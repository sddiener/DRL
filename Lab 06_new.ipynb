{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice dungeon with partial observability\n",
    "\n",
    "Agent A is in a dungeon, and can move up, down, left, right to find the exit:\n",
    "- A agent\n",
    "- E exit\n",
    "- X non-traversable obstacle\n",
    "- L lava.\n",
    "\n",
    "Read the Dungeon class and try to identify how it works, the reward structure, etc.\n",
    "\n",
    "This lab will take inspiration from the PyTorch tutorial on DQN, that we will apply to our custom environment Dungeon.\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "We will adapt the concepts step by step to our environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dungeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dungeon.dungeon import Dungeon, index_to_actions\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_ENVIR = 15\n",
    "\n",
    "my_dungeon = Dungeon(SIZE_ENVIR)\n",
    "my_dungeon.reset()\n",
    "my_dungeon.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dungeon.reset()\n",
    "my_dungeon.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _, _ = my_dungeon.step('left')\n",
    "my_dungeon.display()\n",
    "for obs_name, o in obs.items():\n",
    "    print(obs_name)\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Replay Buffer for Experience replay\n",
    "\n",
    "The first step of this lab is to create a Replay Buffer that will allow us to use Experience Replay and mini-batch learning.\n",
    "- First, we create a class Transition using named tuple, which holds state transition in a dedicated data structure.\n",
    "- Then, create a Replay Memory class that collects transition in a First In First Out fashion (fixed memory size). This Replay memory should convert the states, action, rewards into tensors.\n",
    "\n",
    "The models presented in the Pytorch tutorial are quite generic and can be used as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    \n",
    "    c = state['relative_coordinates'].flatten()/SIZE_ENVIR\n",
    "    o = state['surroundings'].flatten()/4.\n",
    "    state_tensor = np.concatenate( [c,o] )\n",
    "    state_tensor = torch.tensor(state_tensor, device=device).unsqueeze(0)\n",
    "    \n",
    "    return state_tensor\n",
    "    \n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        state_tensor = convert_state(state)\n",
    "        \n",
    "        if next_state is None:\n",
    "            state_tensor_next = None            \n",
    "        else:\n",
    "            state_tensor_next = convert_state(next_state)\n",
    "            \n",
    "        action_tensor = torch.tensor([action], device=device).unsqueeze(0)\n",
    "\n",
    "        reward = torch.tensor([reward], device=device).unsqueeze(0)/10. # reward scaling\n",
    "\n",
    "        self.memory[self.position] = Transition(state_tensor, action_tensor, state_tensor_next, reward)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Q-network\n",
    "\n",
    "A Q-network is a neural network that maps states to Q-values for each actions.\n",
    "\n",
    "Implement a first version of Q-networks.\n",
    "Keep it simple (e.g. 3 hidden layers, with Relu activations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, size_hidden, output_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Declare the different layers\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Declare how the data flows from input x to output.\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Set up the Q-networks\n",
    "\n",
    "In DQN, the weights of the target network are copied from the weights of policy network every few iterations.\n",
    "\n",
    "We set the frequency of update using TARGET_UPDATE = 10.\n",
    "\n",
    "Instead of RMSprop we will use SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_SIZE = 5*5 + 2\n",
    "HIDDEN_SIZE = 64\n",
    "ACTION_SIZE = 4\n",
    "\n",
    "Q_network = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
    "Q_target = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
    "Q_target.load_state_dict(Q_network.state_dict())\n",
    "Q_target.eval()\n",
    "\n",
    "TARGET_UPDATE = 100\n",
    "\n",
    "optimizer = optim.SGD(Q_network.parameters(), lr=0.001)\n",
    "memory = ReplayMemory(10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Epsilon-greedy policy\n",
    "\n",
    "You can take inspiration from pytorch tutorial and implement the select_action function.\n",
    "Or, alternatively, you can implement a E-greedy policy class that will select epsilon greedy actions..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E_Greedy_Policy():\n",
    "    \n",
    "    def __init__(self, epsilon, decay, min_epsilon):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay = decay\n",
    "        self.epsilon_min = min_epsilon\n",
    "                \n",
    "    def __call__(self, state):\n",
    "                \n",
    "        is_greedy = random.random() > self.epsilon\n",
    "        \n",
    "        if is_greedy :\n",
    "            # we select greedy action\n",
    "            with torch.no_grad():\n",
    "                Q_network.eval()\n",
    "                index_action = # take action corresponding to max Q-value\n",
    "                Q_network.train()\n",
    "        else:\n",
    "            # we sample a random action\n",
    "            index_action = random.randint(0,3)\n",
    "        \n",
    "        return index_action\n",
    "                \n",
    "    def update_epsilon(self):\n",
    "        \n",
    "        self.epsilon = self.epsilon*self.decay\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        \n",
    "    def reset(self):\n",
    "        self.epsilon = self.epsilon_start\n",
    "        \n",
    "        \n",
    "policy = E_Greedy_Policy(0.99, decay=0.997, min_epsilon=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.5\n",
    "\n",
    "def optimize_model():\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q values using policy net\n",
    "    Q_values = ...\n",
    "    \n",
    "    # Compute next Q values using Q_targets\n",
    "    next_Q_values = ...\n",
    "    \n",
    "    # Compute targets\n",
    "    target_Q_values = ...\n",
    "    \n",
    "    # Compute MSE Loss\n",
    "    loss = F.mse_loss(Q_values, target_Q_values)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Trick: gradient clipping\n",
    "    for param in Q_network.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "policy.reset()\n",
    "\n",
    "rewards_history = []\n",
    "\n",
    "# Warmup phase!\n",
    "memory_filled = False\n",
    "\n",
    "while not memory_filled:\n",
    "    \n",
    "    my_dungeon = Dungeon(SIZE_ENVIR)\n",
    "    state = my_dungeon.reset()\n",
    "    done = False\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Get action and act in the world\n",
    "        state_tensor = convert_state(state)\n",
    "        \n",
    "        action = policy(state_tensor)\n",
    "        action_name = index_to_actions[action].name \n",
    "        next_state, reward, done = my_dungeon.step(action_name)\n",
    "        \n",
    "        total_reward += float(reward)\n",
    "        \n",
    "        # Observe new state\n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, float(reward))\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "\n",
    "    memory_filled = memory.capacity == len(memory)\n",
    "\n",
    "print('Done with the warmup')\n",
    "    \n",
    "    \n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # New dungeon at every run\n",
    "    my_dungeon = Dungeon(SIZE_ENVIR)\n",
    "    state = my_dungeon.reset()\n",
    "    done = False\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Get action and act in the world\n",
    "        state_tensor = convert_state(state)\n",
    "        \n",
    "        action = policy(state_tensor)\n",
    "        action_name = index_to_actions[action].name \n",
    "        next_state, reward, done = my_dungeon.step(action_name)\n",
    "        \n",
    "        total_reward += float(reward)\n",
    "        \n",
    "        # Observe new state\n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, float(reward))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization\n",
    "        started_training = True\n",
    "        l = optimize_model()\n",
    "\n",
    "    policy.update_epsilon()\n",
    "    rewards_history.append( float(total_reward) )\n",
    "\n",
    "    \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "\n",
    "        Q_target.load_state_dict(Q_network.state_dict())\n",
    "    \n",
    "    if (i_episode) % 10 == 0:\n",
    "        \n",
    "        print('Episode ', i_episode, ': ', 'reward :',  total_reward, 'eps: ', \n",
    "              policy.epsilon, ' loss:', l.detach().cpu())   \n",
    "        print( sum(rewards_history[-10:])/10)    \n",
    "\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards_history, 'b.', alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
