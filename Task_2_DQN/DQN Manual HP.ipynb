{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "print(env.action_space) #[Output: ] Discrete(2)\n",
    "print(env.observation_space) # [Output: ] Box(4,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Replay Buffer for Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def save_transition(self, state, action, next_state, reward):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        state_tensor = T.from_numpy(state)\n",
    "        \n",
    "        if next_state is None:\n",
    "            state_tensor_next = None            \n",
    "        else:\n",
    "            state_tensor_next = T.from_numpy(next_state)\n",
    "        \n",
    "        action_tensor = T.tensor([action], device=device).unsqueeze(0)\n",
    "\n",
    "        reward = T.tensor([reward], device=device).unsqueeze(0)/10. # reward scaling\n",
    "\n",
    "        self.memory[self.position] = Transition(state_tensor, action_tensor, state_tensor_next, reward)  # save the transition\n",
    "        self.position = (self.position + 1) % self.capacity  # loop around memory\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, size_hidden, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, size_hidden)\n",
    "        self.fc2 = nn.Linear(size_hidden, size_hidden)   \n",
    "        self.fc3 = nn.Linear(size_hidden, size_hidden)  \n",
    "        self.fc4 = nn.Linear(size_hidden, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x.float()))  # self.bn1()\n",
    "        h2 = F.relu(self.fc2(h1))  # self.bn2()\n",
    "        h3 = F.relu(self.fc3(h2))  # self.bn3()\n",
    "        output = self.fc4(h3) # .view(h3.size(0), -1)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup phase...\n",
      "Done with the warmup\n",
      "Episode 0 - reward: -288.704, avg. reward (past 10 ep.): -28.870, eps: 0.498 loss: 0.460 \n",
      "Episode 10 - reward: -416.154, avg. reward (past 10 ep.): -234.682, eps: 0.484 loss: 2.286 \n",
      "Complete\n",
      "Warmup phase...\n",
      "Done with the warmup\n",
      "Episode 0 - reward: -148.311, avg. reward (past 10 ep.): -14.831, eps: 0.498 loss: 1.051 \n",
      "Episode 10 - reward: -126.684, avg. reward (past 10 ep.): -172.385, eps: 0.484 loss: 0.047 \n",
      "Complete\n",
      "Warmup phase...\n",
      "Done with the warmup\n",
      "Episode 0 - reward: -368.900, avg. reward (past 10 ep.): -36.890, eps: 0.498 loss: 2.683 \n",
      "Episode 10 - reward: -143.834, avg. reward (past 10 ep.): -191.539, eps: 0.484 loss: 1.028 \n",
      "Complete\n",
      "Warmup phase...\n",
      "Done with the warmup\n",
      "Episode 0 - reward: -101.696, avg. reward (past 10 ep.): -10.170, eps: 0.498 loss: 1.407 \n",
      "Episode 10 - reward: -552.611, avg. reward (past 10 ep.): -173.948, eps: 0.484 loss: 1.096 \n",
      "Complete\n",
      "Warmup phase...\n",
      "Done with the warmup\n",
      "Episode 0 - reward: -345.824, avg. reward (past 10 ep.): -34.582, eps: 0.498 loss: 2.689 \n",
      "Episode 10 - reward: -102.627, avg. reward (past 10 ep.): -185.472, eps: 0.484 loss: 0.021 \n",
      "Complete\n",
      "Warmup phase...\n",
      "Done with the warmup\n",
      "Episode 0 - reward: -198.073, avg. reward (past 10 ep.): -19.807, eps: 0.498 loss: 1.537 \n",
      "Episode 10 - reward: -44.518, avg. reward (past 10 ep.): -250.662, eps: 0.484 loss: 0.486 \n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "rewards_histories = []\n",
    "learning_rates = [0.005, 0.001, 0.0005]\n",
    "memory_sizes = [10000, 100000]\n",
    "i=0\n",
    "for lr in learning_rates:\n",
    "    for ms in memory_sizes:\n",
    "        i+=1\n",
    "        OBS_SIZE = 8\n",
    "        HIDDEN_SIZE = 64\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        Q_network = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
    "        Q_target = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
    "        Q_target.load_state_dict(Q_network.state_dict())\n",
    "        Q_target.eval()\n",
    "\n",
    "        TARGET_UPDATE = 20\n",
    "\n",
    "        optimizer = optim.Adam(Q_network.parameters(), lr=lr)\n",
    "        memory = ReplayMemory(ms)\n",
    "\n",
    "        class E_Greedy_Policy():\n",
    "            \n",
    "            def __init__(self, epsilon, decay, min_epsilon):\n",
    "                self.epsilon = epsilon\n",
    "                self.epsilon_start = epsilon\n",
    "                self.decay = decay\n",
    "                self.epsilon_min = min_epsilon\n",
    "                        \n",
    "            def __call__(self, state):\n",
    "                is_greedy = random.random() > self.epsilon\n",
    "                if is_greedy :\n",
    "                    # we select greedy action\n",
    "                    with T.no_grad():\n",
    "                        Q_network.eval()\n",
    "                        index_action = Q_network(state).argmax().detach().cpu().numpy().item()  # state is on cpu instead of GPU!\n",
    "                        Q_network.train()\n",
    "\n",
    "                else:\n",
    "                    # we sample a random action\n",
    "                    index_action = env.action_space.sample() # select random action (4 possible values)\n",
    "                \n",
    "                return index_action\n",
    "                        \n",
    "            def update_epsilon(self):\n",
    "                \n",
    "                self.epsilon = self.epsilon*self.decay\n",
    "                if self.epsilon < self.epsilon_min:\n",
    "                    self.epsilon = self.epsilon_min\n",
    "                \n",
    "            def reset(self):\n",
    "                self.epsilon = self.epsilon_start\n",
    "\n",
    "        policy = E_Greedy_Policy(epsilon=0.5, decay=0.997, min_epsilon=0.001)\n",
    "        BATCH_SIZE = 64\n",
    "        GAMMA = 0.99\n",
    "\n",
    "        def optimize_model():\n",
    "            transitions = memory.sample(BATCH_SIZE)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            # Compute a mask of non-final states and concatenate the batch elements\n",
    "            non_final_mask = T.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=T.bool)\n",
    "            non_final_next_states = T.cat([s for s in batch.next_state if s is not None])\n",
    "            non_final_next_states = T.reshape(non_final_next_states, (non_final_mask.sum(), -1)).float().to(device)  # Reshape to (nr. non final, 8)\n",
    "\n",
    "            state_batch = T.cat(batch.state).float().to(device)  # .float().to(device) to move to GPU\n",
    "            state_batch = T.reshape(state_batch, (BATCH_SIZE, -1))  # Reshape to (batch_size, 8)\n",
    "            action_batch = T.cat(batch.action).to(device)\n",
    "            reward_batch = T.cat(batch.reward).float().to(device)\n",
    "            \n",
    "            # Compute Q values using policy net\n",
    "            Q_values = Q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "            # Compute next Q values using Q_targets\n",
    "            next_Q_values = T.zeros( BATCH_SIZE, device=device).to(device)\n",
    "            next_Q_values[non_final_mask] = Q_target(non_final_next_states).max(1)[0].detach()\n",
    "            next_Q_values = next_Q_values.unsqueeze(1)\n",
    "            \n",
    "            # Compute targets\n",
    "            target_Q_values = (next_Q_values * GAMMA) + reward_batch\n",
    "            \n",
    "            # Compute MSE Loss\n",
    "            loss = F.mse_loss(Q_values, target_Q_values)\n",
    "            \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Trick: gradient clipping\n",
    "            for param in Q_network.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "            return loss\n",
    "\n",
    "        env = gym.make(\"LunarLander-v2\")  # create environment\n",
    "\n",
    "        num_episodes = 1500\n",
    "        policy.reset()\n",
    "        rewards_history = []\n",
    "\n",
    "        # Warmup phase!\n",
    "        memory_filled = False\n",
    "        print('**************** Parameters: lr={}, ms={} **************** '.format(lr, ms))\n",
    "        print(\"Warmup phase...\")\n",
    "        while not memory_filled:\n",
    "            \n",
    "            state = env.reset()  # 8 states: coordinates of the lander (x,y), linear velocities (x,y), angle, angular velocity, 2 bools if each leg is touches ground.\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:  # for each episode\n",
    "                # Get action and act in the world\n",
    "                state_tensor = T.from_numpy(state).float().to(device)\n",
    "                action = policy(state_tensor)  # <<--- choose greedy (choose index of highest q-value predicted by network) or exploration\n",
    "                next_state, reward, done, __ = env.step(action)\n",
    "                total_reward += float(reward)\n",
    "                \n",
    "                # Observe new state\n",
    "                if done:\n",
    "                    next_state = None\n",
    "\n",
    "                # Store the transition in memory\n",
    "                memory.save_transition(state, action, next_state, float(reward))\n",
    "                state = next_state\n",
    "\n",
    "            memory_filled = memory.capacity == len(memory)\n",
    "\n",
    "        print('Done with the warmup')\n",
    "            \n",
    "        for i_episode in range(num_episodes):\n",
    "            # New dungeon at every run\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            \n",
    "            while not done:  # iterate through states\n",
    "                \n",
    "                # Get action and act in the world\n",
    "                state_tensor = T.from_numpy(state).float().to(device)  # <<--- convert state to tensor and move to GPU\n",
    "                \n",
    "                action = policy(state_tensor)   # choose greedy (index of q-value predictions) or exploration\n",
    "                next_state, reward, done, __ = env.step(action)\n",
    "                total_reward += float(reward)\n",
    "                \n",
    "                # Observe new state\n",
    "                if done:\n",
    "                    next_state = None\n",
    "                memory.save_transition(state, action, next_state, float(reward))  # Store the transition in memory\n",
    "                state = next_state  # Move to the next state\n",
    "\n",
    "                # Perform one step of the optimization\n",
    "                loss = optimize_model()\n",
    "\n",
    "            policy.update_epsilon()\n",
    "            rewards_history.append( float(total_reward) )\n",
    "\n",
    "            \n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                Q_target.load_state_dict(Q_network.state_dict())\n",
    "            \n",
    "            if i_episode % 10 == 0:\n",
    "                avg_rewards_10 = sum(rewards_history[-10:])/10\n",
    "\n",
    "                print('Episode {} - reward: {:.3f}, avg. reward (past 10 ep.): {:.3f}, eps: {:.3f} loss: {:.3f} '.format(\n",
    "                    i_episode, total_reward, avg_rewards_10, policy.epsilon, loss))   \n",
    "        \n",
    "        rewards_histories.append(rewards_history)\n",
    "        data = [i, lr, ms, rewards_histories]\n",
    "        data_list.append(data)\n",
    "        print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_list\n",
    "with open('data_list.pkl', 'wb') as f:\n",
    "    pkl.dump(data_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards_history, '-')\n",
    "# add 100 episode moving average\n",
    "avg_rewards_history = np.convolve(rewards_history, np.ones((100,))/100, mode='valid')\n",
    "plt.plot(avg_rewards_history, '-')\n",
    "plt.title('Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ef810184dd1ed1e7e90889c2154f0f529619798a6f354d5d90c8edd2323b55a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('NC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
