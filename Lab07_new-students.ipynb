{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhRVpY12qvzD",
    "outputId": "0c4fe4f8-596c-42e9-b811-2f8f5480d928"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#path = '/content/gdrive/My Drive/Colab Notebooks/707/Labs/'\n",
    "#import sys\n",
    "#sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XRSA3yCcxoCJ"
   },
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install -U \"ray[rllib]\" torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O5_A5Ok5qySo"
   },
   "outputs": [],
   "source": [
    "from dungeon.dungeon import Dungeon, index_to_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DmyE2zjr8Lj"
   },
   "source": [
    "## Step 1 - Make a Wrapper to use Dungeon with RLLib.\n",
    "\n",
    "Use the following guides:\n",
    "\n",
    "Environment wrapper:\n",
    "https://github.com/ray-project/ray/blob/81dcf9ff351d62da9c7e0493213c01765c9a2534/rllib/examples/custom_env.py#L77\n",
    "\n",
    "Gym spaces:\n",
    "https://gym.openai.com/docs/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GNAS2lX6rtNg"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random, math\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "class DungeonEnv(gym.Env):\n",
    "    \"\"\"Class that wrapps the Dungeon Environment to make it \n",
    "    compatible with RLLib.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EnvContext):\n",
    "\n",
    "        # Get the size from the config.\n",
    "        dungeon_size = config.get(\"size_env\")\n",
    "\n",
    "        # Create a dungeon instance\n",
    "        self.dungeon = Dungeon(N=dungeon_size)\n",
    "\n",
    "        # Define the action spaces\n",
    "        # We will normalize observations between -1 ans 1.\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(dungeon_size, dungeon_size))\n",
    "        \n",
    "    def reset(self):\n",
    "        obs_dungeon = self.dungeon.reset()\n",
    "        obs = self.convert_observations(obs_dungeon)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1, 2, 3]\n",
    "        action_str = index_to_actions[action].name\n",
    "\n",
    "        obs_dungeon, reward, done = self.dungeon.step(action_str)\n",
    "        obs = self.convert_observations(obs_dungeon)\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "    def convert_observations(self, dungeon_obs):\n",
    "        \n",
    "        # We normalize and concatenate observations\n",
    "        # Look into the Dungeon class to know what dungeon obs contains.\n",
    "        \n",
    "        relative_coord = dungeon_obs[\"relative_coordinates\"]\n",
    "        surroundings = dungeon_obs[\"surroundings\"]\n",
    "        \n",
    "        obs = np.concatenate([relative_coord, surroundings])\n",
    "\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QauOX-mbyqp_"
   },
   "source": [
    "## Use DQN and train your algorithm on the Dungeon environment.\n",
    "\n",
    "You can take inspiration from:\n",
    "https://docs.ray.io/en/latest/rllib/rllib-training.html#basic-python-api\n",
    "\n",
    "Experiment with the different parameters of the configuration:\n",
    "https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3SLeKwjxz6S",
    "outputId": "87f03ac8-6a51-4d0d-c2b7-51288864d9d5"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m         \u001b[1;31m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001b[0m in \u001b[0;36m_init\u001b[1;34m(self, config, env_creator)\u001b[0m\n\u001b[0;32m    924\u001b[0m               env_creator: EnvCreator) -> None:\n\u001b[1;32m--> 925\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_24256/3373124011.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDQNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Can optionally call trainer.restore(path) to load a checkpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[0;32m    744\u001b[0m         }\n\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m         super().__init__(config, logger_creator, remote_checkpoint_dir,\n\u001b[0m\u001b[0;32m    747\u001b[0m                          sync_function_tpl)\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\tune\\trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_ip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_current_ip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    820\u001b[0m             \u001b[1;31m# This matches the behavior of using `build_trainer()`, which\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;31m# should no longer be used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m             self.workers = self._make_workers(\n\u001b[0m\u001b[0;32m    823\u001b[0m                 \u001b[0menv_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m                 \u001b[0mvalidate_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001b[0m in \u001b[0;36m_make_workers\u001b[1;34m(self, env_creator, validate_env, policy_class, config, num_workers, local_worker)\u001b[0m\n\u001b[0;32m   1993\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mWorkerSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \"\"\"\n\u001b[1;32m-> 1995\u001b[1;33m         return WorkerSet(\n\u001b[0m\u001b[0;32m   1996\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[0mvalidate_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlocal_worker\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m                 self._local_worker = self._make_worker(\n\u001b[0m\u001b[0;32m    125\u001b[0m                     \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRolloutWorker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                     \u001b[0menv_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m_make_worker\u001b[1;34m(self, cls, env_creator, validate_env, policy_cls, worker_index, num_workers, config, spaces)\u001b[0m\n\u001b[0;32m    498\u001b[0m                 \"extra_python_environs_for_worker\", None)\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         worker = cls(\n\u001b[0m\u001b[0;32m    501\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mvalidate_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, record_env, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, monitor_path)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[1;31m# Validate environment (general validation function).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[0m_validate_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m             \u001b[1;31m# Custom validation function given.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalidate_env\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\u001b[0m in \u001b[0;36m_validate_env\u001b[1;34m(env, env_context)\u001b[0m\n\u001b[0;32m   1710\u001b[0m             env, \"action_space\")\n\u001b[0;32m   1711\u001b[0m         \u001b[1;31m# Get a dummy observation by resetting the env.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1712\u001b[1;33m         \u001b[0mdummy_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1713\u001b[0m         \u001b[1;31m# Convert lists to np.ndarrays.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1714\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy_obs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_24256/3958565977.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mobs_dungeon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdungeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_observations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_dungeon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_24256/3958565977.py\u001b[0m in \u001b[0;36mconvert_observations\u001b[1;34m(self, dungeon_obs)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0msurroundings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdungeon_obs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"surroundings\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrelative_coord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msurroundings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"env\"] = DungeonEnv\n",
    "config[\"env_config\"] = { \"size_env\": 15}\n",
    "\n",
    "# Modify the config to disable dueling, double_q\n",
    "# Use a fully connected network with 2 layers of 64 hidden units.\n",
    "# Use relu activation function\n",
    "#...\n",
    "\n",
    "\n",
    "trainer = dqn.DQNTrainer(config=config)\n",
    "\n",
    "# Can optionally call trainer.restore(path) to load a checkpoint.\n",
    "\n",
    "avg_rewards = []\n",
    "\n",
    "for i in range(100):\n",
    "    # Perform one iteration of training the policy with DQN\n",
    "    result = trainer.train()\n",
    "    #print(pretty_print(result))\n",
    "    print(result['episode_reward_mean'])\n",
    "    avg_rewards.append(result['episode_reward_mean'])\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "JaD__-oszpwu",
    "outputId": "f2aef392-3e19-43bf-fd7a-badb688726c7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(avg_rewards, 'b.', alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3fQhxxi98ECW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
