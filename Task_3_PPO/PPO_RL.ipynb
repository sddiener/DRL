{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19476,"status":"ok","timestamp":1649862677023,"user":{"displayName":"k Yuichi","userId":"17236341909464291729"},"user_tz":-60},"id":"RLAbE_Z-bgQf","outputId":"31c1e9dd-6fd8-4ade-c35a-0240a3dfe78f"},"outputs":[],"source":["# !pip install Box2D\n","# !pip install box2d-py\n","# !pip install gym[all]\n","# !pip install gym[Box_2D]\n","# !pip install torc\n","# !pip install -U \"ray[rllib]\" torch\n","import pickle as pkl\n","import gym \n","env = gym.make(\"LunarLander-v2\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75228,"status":"ok","timestamp":1649863054134,"user":{"displayName":"k Yuichi","userId":"17236341909464291729"},"user_tz":-60},"id":"t1X8pmoNbmNd","outputId":"13da3610-694d-4d21-a85c-78280a0496e3"},"outputs":[{"name":"stderr","output_type":"stream","text":[" pid=8244)\u001b[0m 2022-04-18 18:05:50,632\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n","2022-04-18 18:05:52,239\tINFO trainable.py:125 -- Trainable.setup took 79.601 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n","2022-04-18 18:06:01,484\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n","2022-04-18 18:06:09,249\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"]},{"name":"stdout","output_type":"stream","text":["agent_timesteps_total: 4000\n","custom_metrics: {}\n","date: 2022-04-18_18-06-09\n","done: false\n","episode_len_mean: 92.11627906976744\n","episode_media: {}\n","episode_reward_max: 40.9455968622222\n","episode_reward_mean: -213.21258224279157\n","episode_reward_min: -434.9219179132078\n","episodes_this_iter: 43\n","episodes_total: 43\n","experiment_id: fe5a15e5aa1148abab32edf9fe87eff2\n","hostname: DESKTOP-UETR0GC\n","info:\n","  learner:\n","    default_policy:\n","      custom_metrics: {}\n","      learner_stats:\n","        cur_kl_coeff: 0.20000000298023224\n","        cur_lr: 4.999999873689376e-05\n","        entropy: 1.3766803741455078\n","        entropy_coeff: 0.0\n","        kl: 0.009528086520731449\n","        model: {}\n","        policy_loss: -0.0005107718170620501\n","        total_loss: 11731.158203125\n","        vf_explained_var: -0.0041581797413527966\n","        vf_loss: 11731.15625\n","  num_agent_steps_sampled: 4000\n","  num_agent_steps_trained: 4000\n","  num_steps_sampled: 4000\n","  num_steps_trained: 4000\n","  num_steps_trained_this_iter: 4000\n","iterations_since_restore: 1\n","node_ip: 127.0.0.1\n","num_healthy_workers: 1\n","off_policy_estimator: {}\n","perf:\n","  cpu_util_percent: 39.37619047619047\n","  gpu_util_percent0: 0.24857142857142855\n","  ram_util_percent: 18.04285714285714\n","  vram_util_percent0: 0.28188941592261907\n","pid: 19384\n","policy_reward_max: {}\n","policy_reward_mean: {}\n","policy_reward_min: {}\n","sampler_perf:\n","  mean_action_processing_ms: 0.045774460315585165\n","  mean_env_render_ms: 0.0\n","  mean_env_wait_ms: 0.1691035466383648\n","  mean_inference_ms: 1.9963340621029135\n","  mean_raw_obs_processing_ms: 0.08080077749346234\n","time_since_restore: 16.97743797302246\n","time_this_iter_s: 16.97743797302246\n","time_total_s: 16.97743797302246\n","timers:\n","  learn_throughput: 517.663\n","  learn_time_ms: 7727.027\n","  load_throughput: 0.0\n","  load_time_ms: 0.0\n","  sample_throughput: 432.654\n","  sample_time_ms: 9245.271\n","  update_time_ms: 2.002\n","timestamp: 1650301569\n","timesteps_since_restore: 4000\n","timesteps_this_iter: 4000\n","timesteps_total: 4000\n","training_iteration: 1\n","trial_id: default\n","\n","checkpoint saved at C:\\Users\\stefan/ray_results\\PPOTrainer_LunarLander-v2_2022-04-18_18-04-32u_6zbdcb\\checkpoint_000001\\checkpoint-1\n","agent_timesteps_total: 8000\n","custom_metrics: {}\n","date: 2022-04-18_18-06-26\n","done: false\n","episode_len_mean: 104.35526315789474\n","episode_media: {}\n","episode_reward_max: 124.57727342810196\n","episode_reward_mean: -180.63935849401398\n","episode_reward_min: -434.9219179132078\n","episodes_this_iter: 33\n","episodes_total: 76\n","experiment_id: fe5a15e5aa1148abab32edf9fe87eff2\n","hostname: DESKTOP-UETR0GC\n","info:\n","  learner:\n","    default_policy:\n","      custom_metrics: {}\n","      learner_stats:\n","        cur_kl_coeff: 0.20000000298023224\n","        cur_lr: 4.999999873689376e-05\n","        entropy: 1.3724448680877686\n","        entropy_coeff: 0.0\n","        kl: 0.008845928125083447\n","        model: {}\n","        policy_loss: -0.01001485250890255\n","        total_loss: 3843.341552734375\n","        vf_explained_var: 0.06437209993600845\n","        vf_loss: 3843.35009765625\n","  num_agent_steps_sampled: 8000\n","  num_agent_steps_trained: 8000\n","  num_steps_sampled: 8000\n","  num_steps_trained: 8000\n","  num_steps_trained_this_iter: 4000\n","iterations_since_restore: 2\n","node_ip: 127.0.0.1\n","num_healthy_workers: 1\n","off_policy_estimator: {}\n","perf:\n","  cpu_util_percent: 47.647619047619045\n","  gpu_util_percent0: 0.25190476190476196\n","  ram_util_percent: 17.95714285714286\n","  vram_util_percent0: 0.28073846726190477\n","pid: 19384\n","policy_reward_max: {}\n","policy_reward_mean: {}\n","policy_reward_min: {}\n","sampler_perf:\n","  mean_action_processing_ms: 0.046049799253582234\n","  mean_env_render_ms: 0.0\n","  mean_env_wait_ms: 0.20707452484301606\n","  mean_inference_ms: 1.9942192464802417\n","  mean_raw_obs_processing_ms: 0.08102468329288132\n","time_since_restore: 34.12102770805359\n","time_this_iter_s: 17.143589735031128\n","time_total_s: 34.12102770805359\n","timers:\n","  learn_throughput: 533.845\n","  learn_time_ms: 7492.814\n","  load_throughput: 0.0\n","  load_time_ms: 0.0\n","  sample_throughput: 297.214\n","  sample_time_ms: 13458.333\n","  update_time_ms: 2.002\n","timestamp: 1650301586\n","timesteps_since_restore: 8000\n","timesteps_this_iter: 4000\n","timesteps_total: 8000\n","training_iteration: 2\n","trial_id: default\n","\n"]}],"source":["import ray\n","import ray.rllib.agents.ppo as ppo\n","from ray.tune.logger import pretty_print\n","\n","config = ppo.DEFAULT_CONFIG.copy()\n","config[\"num_gpus\"] = 0\n","config[\"num_workers\"] = 1\n","trainer = ppo.PPOTrainer(config=config, env=\"LunarLander-v2\")\n","\n","# Can optionally call trainer.restore(path) to load a checkpoint.\n","\n","for i in range(2):\n","   # Perform one iteration of training the policy with PPO\n","   result = trainer.train()\n","   print(pretty_print(result))\n","\n","   if i % 2 == 0:\n","       checkpoint = trainer.save()\n","       print(\"checkpoint saved at\", checkpoint)\n","\n","# Also, in case you have trained a model outside of ray/RLlib and have created\n","# an h5-file with weight values in it, e.g.\n","# my_keras_model_trained_outside_rllib.save_weights(\"model.h5\")\n","# (see: https://keras.io/models/about-keras-models/)\n","\n","# ... you can load the h5-weights into your Trainer's Policy's ModelV2\n","# (tf or torch) by doing:\n","\n","# NOTE: In order for this to work, your (custom) model needs to implement\n","# the `import_from_h5` method.\n","# See https://github.com/ray-project/ray/blob/master/rllib/tests/test_model_imports.py\n","# for detailed examples for tf- and torch trainers/models."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1354535,"status":"ok","timestamp":1649864415977,"user":{"displayName":"k Yuichi","userId":"17236341909464291729"},"user_tz":-60},"id":"UEFWSr9EcQEg","outputId":"fa03064c-029b-48dd-ac51-817e8403001b"},"outputs":[{"data":{"text/html":["== Status ==<br>Current time: 2022-04-19 08:59:53 (running for 14:53:26.50)<br>Memory usage on this node: 18.5/64.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/33.09 GiB heap, 0.0/16.54 GiB objects<br>Current best trial: e1da1_00002 with episode_reward_mean=217.1938642855226 and parameters={'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'LunarLander-v2', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'LunarLander-v2', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: C:\\Users\\stefan\\ray_results\\PPO<br>Number of trials: 3/3 (3 TERMINATED)<br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-04-19 08:59:53,675\tINFO tune.py:636 -- Total run time: 53607.19 seconds (53606.48 seconds for the tuning loop).\n"]}],"source":["import ray\n","from ray import tune\n","\n","analysis = tune.run(\n","    \"PPO\",\n","    metric=\"episode_reward_mean\",\n","    mode=\"max\",\n","    stop={\"training_iteration\": 2000},\n","    config={\n","        \"env\": \"LunarLander-v2\",\n","        \"num_gpus\": 0,\n","        #\"num_workers\": 1,\n","        \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n","    },\n","    verbose=1,\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"LJ8pqXHfdicB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'env': 'LunarLander-v2', 'num_gpus': 0, 'lr': 0.0001}\n"]}],"source":["print(\"Best hyperparameters found were: \", analysis.best_config)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>episode_reward_mean</th>\n","      <th>episode_reward_max</th>\n","      <th>config/lr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-132.069360</td>\n","      <td>26.204158</td>\n","      <td>0.0100</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>200.376501</td>\n","      <td>290.861195</td>\n","      <td>0.0010</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>282.199921</td>\n","      <td>317.006130</td>\n","      <td>0.0001</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   episode_reward_mean  episode_reward_max  config/lr\n","0          -132.069360           26.204158     0.0100\n","1           200.376501          290.861195     0.0010\n","2           282.199921          317.006130     0.0001"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")[['episode_reward_mean', 'episode_reward_max', 'config/lr']]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# save analysis to file\n","with open(\"analysis_finalDFs_PPO.pkl\", \"wb\") as f:\n","    pkl.dump(analysis.dataframe(), f)\n","\n","with open(\"analysis_trialDFs_PPO.pkl\", \"wb\") as f:\n","    pkl.dump(analysis.trial_dataframes, f)\n","\n","with open(\"analysis_configs_PPO.pkl\", \"wb\") as f:\n","    pkl.dump(analysis._configs, f)"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyMFc8t2L9hfzg2Me42v/zNv","collapsed_sections":[],"machine_shape":"hm","name":"PPO_RL","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
