{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19476,"status":"ok","timestamp":1649862677023,"user":{"displayName":"k Yuichi","userId":"17236341909464291729"},"user_tz":-60},"id":"RLAbE_Z-bgQf","outputId":"31c1e9dd-6fd8-4ade-c35a-0240a3dfe78f"},"outputs":[],"source":["# !pip install Box2D\n","# !pip install box2d-py\n","# !pip install gym[all]\n","# !pip install gym[Box_2D]\n","# !pip install torc\n","# !pip install -U \"ray[rllib]\" torch\n","import pickle as pkl\n","import gym \n","env = gym.make(\"LunarLander-v2\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75228,"status":"ok","timestamp":1649863054134,"user":{"displayName":"k Yuichi","userId":"17236341909464291729"},"user_tz":-60},"id":"t1X8pmoNbmNd","outputId":"13da3610-694d-4d21-a85c-78280a0496e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-04-18 16:49:03,979\tINFO trainable.py:127 -- Trainable.setup took 13.890 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n","2022-04-18 16:49:03,982\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"]},{"name":"stdout","output_type":"stream","text":["agent_timesteps_total: 4000\n","custom_metrics: {}\n","date: 2022-04-18_16-49-12\n","done: false\n","episode_len_mean: 94.38095238095238\n","episode_media: {}\n","episode_reward_max: -49.521158445305524\n","episode_reward_mean: -176.6025630053101\n","episode_reward_min: -460.596531980081\n","episodes_this_iter: 42\n","episodes_total: 42\n","experiment_id: 1388888b1443412ebd4aa5949d8357c4\n","hostname: STEFANXPS\n","info:\n","  learner:\n","    default_policy:\n","      custom_metrics: {}\n","      learner_stats:\n","        cur_kl_coeff: 0.20000000298023224\n","        cur_lr: 4.999999873689376e-05\n","        entropy: 1.3703820705413818\n","        entropy_coeff: 0.0\n","        kl: 0.016125040128827095\n","        model: {}\n","        policy_loss: -0.006196949630975723\n","        total_loss: 8762.4814453125\n","        vf_explained_var: -0.005872588604688644\n","        vf_loss: 8762.484375\n","  num_agent_steps_sampled: 4000\n","  num_agent_steps_trained: 4000\n","  num_steps_sampled: 4000\n","  num_steps_trained: 4000\n","  num_steps_trained_this_iter: 4000\n","iterations_since_restore: 1\n","node_ip: 127.0.0.1\n","num_healthy_workers: 1\n","off_policy_estimator: {}\n","perf:\n","  cpu_util_percent: 40.176923076923075\n","  ram_util_percent: 59.70769230769231\n","pid: 12920\n","policy_reward_max: {}\n","policy_reward_mean: {}\n","policy_reward_min: {}\n","sampler_perf:\n","  mean_action_processing_ms: 0.05113437931944627\n","  mean_env_render_ms: 0.0\n","  mean_env_wait_ms: 0.1936992148523538\n","  mean_inference_ms: 0.9876068041819805\n","  mean_raw_obs_processing_ms: 0.09974596709556743\n","time_since_restore: 8.771626710891724\n","time_this_iter_s: 8.771626710891724\n","time_total_s: 8.771626710891724\n","timers:\n","  learn_throughput: 1201.583\n","  learn_time_ms: 3328.942\n","  load_throughput: 0.0\n","  load_time_ms: 0.0\n","  sample_throughput: 735.786\n","  sample_time_ms: 5436.364\n","  update_time_ms: 2.22\n","timestamp: 1650296952\n","timesteps_since_restore: 4000\n","timesteps_this_iter: 4000\n","timesteps_total: 4000\n","training_iteration: 1\n","trial_id: default\n","\n","checkpoint saved at C:\\Users\\sdien/ray_results\\PPOTrainer_LunarLander-v2_2022-04-18_16-48-50wyanr3xd\\checkpoint_000001\\checkpoint-1\n","agent_timesteps_total: 8000\n","custom_metrics: {}\n","date: 2022-04-18_16-49-25\n","done: false\n","episode_len_mean: 91.57471264367815\n","episode_media: {}\n","episode_reward_max: -18.398200333202524\n","episode_reward_mean: -157.09530748021268\n","episode_reward_min: -460.596531980081\n","episodes_this_iter: 45\n","episodes_total: 87\n","experiment_id: 1388888b1443412ebd4aa5949d8357c4\n","hostname: STEFANXPS\n","info:\n","  learner:\n","    default_policy:\n","      custom_metrics: {}\n","      learner_stats:\n","        cur_kl_coeff: 0.20000000298023224\n","        cur_lr: 4.999999873689376e-05\n","        entropy: 1.3675925731658936\n","        entropy_coeff: 0.0\n","        kl: 0.004822745453566313\n","        model: {}\n","        policy_loss: 0.008243076503276825\n","        total_loss: 4297.81396484375\n","        vf_explained_var: -0.0022035865113139153\n","        vf_loss: 4297.8046875\n","  num_agent_steps_sampled: 8000\n","  num_agent_steps_trained: 8000\n","  num_steps_sampled: 8000\n","  num_steps_trained: 8000\n","  num_steps_trained_this_iter: 4000\n","iterations_since_restore: 2\n","node_ip: 127.0.0.1\n","num_healthy_workers: 1\n","off_policy_estimator: {}\n","perf:\n","  cpu_util_percent: 40.150000000000006\n","  ram_util_percent: 59.6888888888889\n","pid: 12920\n","policy_reward_max: {}\n","policy_reward_mean: {}\n","policy_reward_min: {}\n","sampler_perf:\n","  mean_action_processing_ms: 0.060231410367363986\n","  mean_env_render_ms: 0.0\n","  mean_env_wait_ms: 0.2423599563013134\n","  mean_inference_ms: 1.1716386217777612\n","  mean_raw_obs_processing_ms: 0.13081034791744717\n","time_since_restore: 21.844032049179077\n","time_this_iter_s: 13.072405338287354\n","time_total_s: 21.844032049179077\n","timers:\n","  learn_throughput: 1191.517\n","  learn_time_ms: 3357.066\n","  load_throughput: 8029296.961\n","  load_time_ms: 0.498\n","  sample_throughput: 430.646\n","  sample_time_ms: 9288.373\n","  update_time_ms: 2.159\n","timestamp: 1650296965\n","timesteps_since_restore: 8000\n","timesteps_this_iter: 4000\n","timesteps_total: 8000\n","training_iteration: 2\n","trial_id: default\n","\n","agent_timesteps_total: 12000\n","custom_metrics: {}\n","date: 2022-04-18_16-49-34\n","done: false\n","episode_len_mean: 91.96\n","episode_media: {}\n","episode_reward_max: -5.275875916731337\n","episode_reward_mean: -136.7099197281475\n","episode_reward_min: -440.7666517213587\n","episodes_this_iter: 44\n","episodes_total: 131\n","experiment_id: 1388888b1443412ebd4aa5949d8357c4\n","hostname: STEFANXPS\n","info:\n","  learner:\n","    default_policy:\n","      custom_metrics: {}\n","      learner_stats:\n","        cur_kl_coeff: 0.10000000149011612\n","        cur_lr: 4.999999873689376e-05\n","        entropy: 1.3433173894882202\n","        entropy_coeff: 0.0\n","        kl: 0.010005520656704903\n","        model: {}\n","        policy_loss: -0.007410306483507156\n","        total_loss: 3008.272216796875\n","        vf_explained_var: -0.004176571499556303\n","        vf_loss: 3008.27880859375\n","  num_agent_steps_sampled: 12000\n","  num_agent_steps_trained: 12000\n","  num_steps_sampled: 12000\n","  num_steps_trained: 12000\n","  num_steps_trained_this_iter: 4000\n","iterations_since_restore: 3\n","node_ip: 127.0.0.1\n","num_healthy_workers: 1\n","off_policy_estimator: {}\n","perf:\n","  cpu_util_percent: 41.3\n","  ram_util_percent: 59.79230769230768\n","pid: 12920\n","policy_reward_max: {}\n","policy_reward_mean: {}\n","policy_reward_min: {}\n","sampler_perf:\n","  mean_action_processing_ms: 0.06516111396153745\n","  mean_env_render_ms: 0.0\n","  mean_env_wait_ms: 0.2641655017756416\n","  mean_inference_ms: 1.2560069108254814\n","  mean_raw_obs_processing_ms: 0.1461506993472385\n","time_since_restore: 30.722285747528076\n","time_this_iter_s: 8.878253698348999\n","time_total_s: 30.722285747528076\n","timers:\n","  learn_throughput: 1201.785\n","  learn_time_ms: 3328.383\n","  load_throughput: 9525292.96\n","  load_time_ms: 0.42\n","  sample_throughput: 434.796\n","  sample_time_ms: 9199.707\n","  update_time_ms: 2.092\n","timestamp: 1650296974\n","timesteps_since_restore: 12000\n","timesteps_this_iter: 4000\n","timesteps_total: 12000\n","training_iteration: 3\n","trial_id: default\n","\n","checkpoint saved at C:\\Users\\sdien/ray_results\\PPOTrainer_LunarLander-v2_2022-04-18_16-48-50wyanr3xd\\checkpoint_000003\\checkpoint-3\n","agent_timesteps_total: 16000\n","custom_metrics: {}\n","date: 2022-04-18_16-49-44\n","done: false\n","episode_len_mean: 90.11\n","episode_media: {}\n","episode_reward_max: -5.275875916731337\n","episode_reward_mean: -125.01874573989015\n","episode_reward_min: -406.49660394091757\n","episodes_this_iter: 45\n","episodes_total: 176\n","experiment_id: 1388888b1443412ebd4aa5949d8357c4\n","hostname: STEFANXPS\n","info:\n","  learner:\n","    default_policy:\n","      custom_metrics: {}\n","      learner_stats:\n","        cur_kl_coeff: 0.10000000149011612\n","        cur_lr: 4.999999873689376e-05\n","        entropy: 1.3469136953353882\n","        entropy_coeff: 0.0\n","        kl: 0.020242678001523018\n","        model: {}\n","        policy_loss: -0.010520502924919128\n","        total_loss: 3508.619873046875\n","        vf_explained_var: -0.007998524233698845\n","        vf_loss: 3508.628173828125\n","  num_agent_steps_sampled: 16000\n","  num_agent_steps_trained: 16000\n","  num_steps_sampled: 16000\n","  num_steps_trained: 16000\n","  num_steps_trained_this_iter: 4000\n","iterations_since_restore: 4\n","node_ip: 127.0.0.1\n","num_healthy_workers: 1\n","off_policy_estimator: {}\n","perf:\n","  cpu_util_percent: 37.96153846153846\n","  ram_util_percent: 59.85384615384615\n","pid: 12920\n","policy_reward_max: {}\n","policy_reward_mean: {}\n","policy_reward_min: {}\n","sampler_perf:\n","  mean_action_processing_ms: 0.06540224956807508\n","  mean_env_render_ms: 0.0\n","  mean_env_wait_ms: 0.2576482906650185\n","  mean_inference_ms: 1.2254729457631413\n","  mean_raw_obs_processing_ms: 0.14263807937079442\n","time_since_restore: 39.90644454956055\n","time_this_iter_s: 9.18415880203247\n","time_total_s: 39.90644454956055\n","timers:\n","  learn_throughput: 1209.556\n","  learn_time_ms: 3306.999\n","  load_throughput: 7107484.008\n","  load_time_ms: 0.563\n","  sample_throughput: 434.156\n","  sample_time_ms: 9213.283\n","  update_time_ms: 2.307\n","timestamp: 1650296984\n","timesteps_since_restore: 16000\n","timesteps_this_iter: 4000\n","timesteps_total: 16000\n","training_iteration: 4\n","trial_id: default\n","\n"]}],"source":["import ray\n","import ray.rllib.agents.ppo as ppo\n","from ray.tune.logger import pretty_print\n","\n","config = ppo.DEFAULT_CONFIG.copy()\n","config[\"num_gpus\"] = 0\n","config[\"num_workers\"] = 1\n","trainer = ppo.PPOTrainer(config=config, env=\"LunarLander-v2\")\n","\n","# Can optionally call trainer.restore(path) to load a checkpoint.\n","\n","for i in range(2):\n","   # Perform one iteration of training the policy with PPO\n","   result = trainer.train()\n","   print(pretty_print(result))\n","\n","   if i % 2 == 0:\n","       checkpoint = trainer.save()\n","       print(\"checkpoint saved at\", checkpoint)\n","\n","# Also, in case you have trained a model outside of ray/RLlib and have created\n","# an h5-file with weight values in it, e.g.\n","# my_keras_model_trained_outside_rllib.save_weights(\"model.h5\")\n","# (see: https://keras.io/models/about-keras-models/)\n","\n","# ... you can load the h5-weights into your Trainer's Policy's ModelV2\n","# (tf or torch) by doing:\n","\n","# NOTE: In order for this to work, your (custom) model needs to implement\n","# the `import_from_h5` method.\n","# See https://github.com/ray-project/ray/blob/master/rllib/tests/test_model_imports.py\n","# for detailed examples for tf- and torch trainers/models."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1354535,"status":"ok","timestamp":1649864415977,"user":{"displayName":"k Yuichi","userId":"17236341909464291729"},"user_tz":-60},"id":"UEFWSr9EcQEg","outputId":"fa03064c-029b-48dd-ac51-817e8403001b"},"outputs":[{"data":{"text/html":["== Status ==<br>Current time: 2022-04-18 16:55:41 (running for 00:01:26.19)<br>Memory usage on this node: 10.5/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Current best trial: cbff5_00001 with episode_reward_mean=-123.20411564603299 and parameters={'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'LunarLander-v2', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'LunarLander-v2', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: C:\\Users\\sdien\\ray_results\\PPO<br>Number of trials: 3/3 (3 TERMINATED)<br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-04-18 16:55:41,287\tINFO tune.py:639 -- Total run time: 86.44 seconds (86.18 seconds for the tuning loop).\n"]}],"source":["import ray\n","from ray import tune\n","\n","analysis = tune.run(\n","    \"PPO\",\n","    metric=\"episode_reward_mean\",\n","    mode=\"max\",\n","    stop={\"training_iteration\": 3},\n","    config={\n","        \"env\": \"LunarLander-v2\",\n","        \"num_gpus\": 0,\n","        #\"num_workers\": 1,\n","        \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n","    },\n","    verbose=1,\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"LJ8pqXHfdicB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters found were:  {'env': 'LunarLander-v2', 'num_gpus': 0, 'lr': 0.001}\n"]}],"source":["print(\"Best hyperparameters found were: \", analysis.best_config)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>episode_reward_mean</th>\n","      <th>episode_reward_max</th>\n","      <th>config/lr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-183.608621</td>\n","      <td>-6.156078</td>\n","      <td>0.0100</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-123.204116</td>\n","      <td>31.522609</td>\n","      <td>0.0010</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-124.371005</td>\n","      <td>-8.912454</td>\n","      <td>0.0001</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   episode_reward_mean  episode_reward_max  config/lr\n","0          -183.608621           -6.156078     0.0100\n","1          -123.204116           31.522609     0.0010\n","2          -124.371005           -8.912454     0.0001"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")[['episode_reward_mean', 'episode_reward_max', 'config/lr']]"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# save analysis to file\n","with open(\"analysis_finalDFs_PPO.pkl\", \"wb\") as f:\n","    pkl.dump(analysis.dataframe(), f)\n","\n","with open(\"analysis_trialDFs_PPO.pkl\", \"wb\") as f:\n","    pkl.dump(analysis.trial_dataframes, f)\n","\n","with open(\"analysis_configs_PPO.pkl\", \"wb\") as f:\n","    pkl.dump(analysis._configs, f)"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyMFc8t2L9hfzg2Me42v/zNv","collapsed_sections":[],"machine_shape":"hm","name":"PPO_RL","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
