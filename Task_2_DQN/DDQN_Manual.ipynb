{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDQN_Manual.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2hi5yIiLUUm",
        "outputId": "313ce561-d770-4bb5-f5e5-ead092dea75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n",
            "Requirement already satisfied: ribs[all] in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: gym~=0.17.0 in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: Box2D~=2.3.10 in /usr/local/lib/python3.7/dist-packages (2.3.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.0) (0.16.0)\n",
            "Requirement already satisfied: toml>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (0.10.2)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (2.4.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (0.51.2)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (4.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->ribs[all]) (4.1.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs[all]) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs[all]) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->ribs[all]) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ribs[all]) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ribs[all]) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "%pip install ribs[all] gym~=0.17.0 Box2D~=2.3.10 tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time \n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# plotting\n",
        "%matplotlib inline\n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "i4Dyg0fFLlv1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "print(env.action_space) #[Output: ] Discrete(2)\n",
        "print(env.observation_space) # [Output: ] Box(4,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yRzpVL2LZp0",
        "outputId": "2d1f7216-778a-4348-f6e7-9eacf1a90a03"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n",
            "Box(-inf, inf, (8,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
      ],
      "metadata": {
        "id": "-lioBTfFLg3U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def save_transition(self, state, action, next_state, reward):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        \n",
        "        state_tensor = T.from_numpy(state)\n",
        "        \n",
        "        if next_state is None:\n",
        "            state_tensor_next = None            \n",
        "        else:\n",
        "            state_tensor_next = T.from_numpy(next_state)\n",
        "        \n",
        "        action_tensor = T.tensor([action], device=device).unsqueeze(0)\n",
        "\n",
        "        reward = T.tensor([reward], device=device).unsqueeze(0)/10. # reward scaling\n",
        "\n",
        "        self.memory[self.position] = Transition(state_tensor, action_tensor, state_tensor_next, reward)  # save the transition\n",
        "        self.position = (self.position + 1) % self.capacity  # loop around memory\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "8Xnqgkq5L3_7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if gpu is to be used\n",
        "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, size_hidden, output_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, size_hidden)\n",
        "        self.fc2 = nn.Linear(size_hidden, size_hidden)   \n",
        "        self.fc3 = nn.Linear(size_hidden, size_hidden)  \n",
        "        self.fc4 = nn.Linear(size_hidden, output_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        h1 = F.relu(self.fc1(x.float()))  # self.bn1()\n",
        "        h2 = F.relu(self.fc2(h1))  # self.bn2()\n",
        "        h3 = F.relu(self.fc3(h2))  # self.bn3()\n",
        "        output = self.fc4(h3) # .view(h3.size(0), -1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "lP_u7YpKL-e-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OBS_SIZE = 8\n",
        "HIDDEN_SIZE = 64\n",
        "ACTION_SIZE = 4\n",
        "\n",
        "Q_network = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
        "Q_target = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
        "Q_target.load_state_dict(Q_network.state_dict())\n",
        "Q_target.eval()\n",
        "\n",
        "TARGET_UPDATE = 20\n",
        "\n",
        "optimizer = optim.Adam(Q_network.parameters(), lr=0.001)\n",
        "memory = ReplayMemory(500000)"
      ],
      "metadata": {
        "id": "J5-I8mIVMAdc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class E_Greedy_Policy():\n",
        "    \n",
        "    def __init__(self, epsilon, decay, min_epsilon):\n",
        "        \n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_start = epsilon\n",
        "        self.decay = decay\n",
        "        self.epsilon_min = min_epsilon\n",
        "                \n",
        "    def __call__(self, state):\n",
        "                \n",
        "        is_greedy = random.random() > self.epsilon\n",
        "        if is_greedy :\n",
        "            # we select greedy action\n",
        "            with T.no_grad():\n",
        "                Q_network.eval()\n",
        "                index_action = Q_network(state).argmax().detach().cpu().numpy().item()  # state is on cpu instead of GPU!\n",
        "                Q_network.train()\n",
        "\n",
        "        else:\n",
        "            # we sample a random action\n",
        "            index_action = env.action_space.sample() # select random action (4 possible values)\n",
        "        \n",
        "        return index_action\n",
        "                \n",
        "    def update_epsilon(self):\n",
        "        \n",
        "        self.epsilon = self.epsilon*self.decay\n",
        "        if self.epsilon < self.epsilon_min:\n",
        "            self.epsilon = self.epsilon_min\n",
        "        \n",
        "    def reset(self):\n",
        "        self.epsilon = self.epsilon_start"
      ],
      "metadata": {
        "id": "H9-nybIwMD15"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = E_Greedy_Policy(epsilon=0.5, decay=0.997, min_epsilon=0.001)\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "\n",
        "def optimize_model():\n",
        "    \n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    non_final_mask = T.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=T.bool)\n",
        "    non_final_next_states = T.cat([s for s in batch.next_state if s is not None])\n",
        "    non_final_next_states = T.reshape(non_final_next_states, (non_final_mask.sum(), -1)).float().to(device)  # Reshape to (nr. non final, 8)\n",
        "\n",
        "    state_batch = T.cat(batch.state).float().to(device)  # .float().to(device) to move to GPU\n",
        "    state_batch = T.reshape(state_batch, (BATCH_SIZE, -1))  # Reshape to (batch_size, 8)\n",
        "    action_batch = T.cat(batch.action).to(device)\n",
        "    reward_batch = T.cat(batch.reward).float().to(device)\n",
        "    \n",
        "\n",
        "  \n",
        "    # Compute Q values using policy net\n",
        "    Q_values = Q_network(state_batch).gather(1, action_batch)\n",
        "    # Compute next Q values using Q_targets\n",
        "    next_Q_values = T.zeros( BATCH_SIZE, device=device).to(device)\n",
        "\n",
        "\n",
        "    # DDQN Implementation \n",
        "    ddqn_idx = Q_network(non_final_next_states).argmax(dim=1, keepdim=True)\n",
        "\n",
        "    print(ddqn_idx.shape)\n",
        "    print(Q_target(non_final_next_states).shape)\n",
        "    next_Q_values[non_final_mask] = Q_target(non_final_next_states).gather(\n",
        "        1, ddqn_idx)#.detach()\n",
        "    next_Q_values = next_Q_values.unsqueeze(1)\n",
        "\n",
        "        # Compute targets\n",
        "    target_Q_values = (next_Q_values * GAMMA) + reward_batch\n",
        "\n",
        "    \n",
        "    # Compute targets\n",
        "\n",
        "    target_Q_values = ()\n",
        "    \n",
        "    # Compute MSE Loss\n",
        "    loss = F.mse_loss(Q_values, target_Q_values)\n",
        "    \n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # Trick: gradient clipping\n",
        "    for param in Q_network.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "        \n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss"
      ],
      "metadata": {
        "id": "elxOmAbdMF0E"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")  # create environment\n",
        "\n",
        "num_episodes = 2\n",
        "policy.reset()\n",
        "rewards_history = []\n",
        "\n",
        "# Warmup phase!\n",
        "memory_filled = False\n",
        "print(\"Warmup phase...\")\n",
        "while not memory_filled:\n",
        "    \n",
        "    state = env.reset()  # 8 states: coordinates of the lander (x,y), linear velocities (x,y), angle, angular velocity, 2 bools if each leg is touches ground.\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    \n",
        "    while not done:  # for each episode\n",
        "        # Get action and act in the world\n",
        "        state_tensor = T.from_numpy(state).float().to(device)\n",
        "        action = policy(state_tensor)  # <<--- choose greedy (choose index of highest q-value predicted by network) or exploration\n",
        "        next_state, reward, done, __ = env.step(action)\n",
        "        total_reward += float(reward)\n",
        "        \n",
        "\n",
        "               # Observe new state\n",
        "        if done:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.save_transition(state, action, next_state, float(reward))\n",
        "        state = next_state\n",
        "\n",
        "    memory_filled = memory.capacity == len(memory)\n",
        "\n",
        "print('Done with the warmup')\n",
        "    \n",
        "for i_episode in range(num_episodes):\n",
        "    # New dungeon at every run\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    \n",
        "    \n",
        "    while not done:  # iterate through states\n",
        "        \n",
        "        # Get action and act in the world\n",
        "        state_tensor = T.from_numpy(state).float().to(device)  # <<--- convert state to tensor and move to GPU\n",
        "        \n",
        "        action = policy(state_tensor)   # choose greedy (index of q-value predictions) or exploration\n",
        "        next_state, reward, done, __ = env.step(action)\n",
        "        total_reward += float(reward)\n",
        "\n",
        "\n",
        "                # Observe new state\n",
        "        if done:\n",
        "            next_state = None\n",
        "        memory.save_transition(state, action, next_state, float(reward))  # Store the transition in memory\n",
        "        state = next_state  # Move to the next state\n",
        "\n",
        "        # Perform one step of the optimization\n",
        "        #started_training = True\n",
        "        loss = optimize_model()\n",
        "\n",
        "    policy.update_epsilon()\n",
        "    rewards_history.append( float(total_reward) )\n",
        "\n",
        "    \n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        Q_target.load_state_dict(Q_network.state_dict())\n",
        "    \n",
        "    if i_episode % 10 == 0:\n",
        "        avg_rewards_10 = sum(rewards_history[-10:])/10\n",
        "\n",
        "        print('Episode {} - reward: {:.3f}, avg. reward (past 10 ep.): {:.3f}, eps: {:.3f} loss: {:.3f} '.format(\n",
        "            i_episode, total_reward, avg_rewards_10, policy.epsilon, loss))   \n",
        "    \n",
        "print('Complete')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "j1Yj8kG5ML8Z",
        "outputId": "d7b593e9-ea6f-4fed-9640-4d2fac2ba761"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup phase...\n",
            "Done with the warmup\n",
            "torch.Size([64, 1])\n",
            "torch.Size([64, 4])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-d1e1e7ff897d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Perform one step of the optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#started_training = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-cfb384342175>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     next_Q_values[non_final_mask] = Q_target(non_final_next_states).gather(\n\u001b[0;32m---> 34\u001b[0;31m         1, action_batch)#.detach()\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mnext_Q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_Q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [64, 1] cannot be broadcast to indexing result of shape [64]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rewards_history, '-')\n",
        "# add 100 episode moving average\n",
        "avg_rewards_history = np.convolve(rewards_history, np.ones((100,))/100, mode='valid')\n",
        "plt.plot(avg_rewards_history, '-')\n",
        "plt.title('Rewards')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dq8CeLiuMVHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "0R-3_LoeNIWi",
        "outputId": "ff3f5f9f-fdda-49f5-9afd-016d86889934"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-38114a9f29b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'non_final_next_states' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FFrQMresuYsm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}