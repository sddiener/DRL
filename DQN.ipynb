{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    new_observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    arr = env.render(mode=\"rgb_array\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space) #[Output: ] Discrete(2)\n",
    "print(env.observation_space) # [Output: ] Box(4,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Replay Buffer for Experience replay\n",
    "\n",
    "The first step of this lab is to create a Replay Buffer that will allow us to use Experience Replay and mini-batch learning.\n",
    "- First, we create a class Transition using named tuple, which holds state transition in a dedicated data structure.\n",
    "- Then, create a Replay Memory class that collects transition in a First In First Out fashion (fixed memory size). This Replay memory should convert the states, action, rewards into tensors.\n",
    "\n",
    "The models presented in the Pytorch tutorial are quite generic and can be used as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        state_tensor = T.from_numpy(state)\n",
    "        \n",
    "        if next_state is None:\n",
    "            state_tensor_next = None            \n",
    "        else:\n",
    "            state_tensor_next = T.from_numpy(next_state)\n",
    "        \n",
    "        action_tensor = torch.tensor([action], device=device).unsqueeze(0)\n",
    "\n",
    "        reward = torch.tensor([reward], device=device).unsqueeze(0)/10. # reward scaling\n",
    "\n",
    "        self.memory[self.position] = Transition(state_tensor, action_tensor, state_tensor_next, reward)  # save the transition\n",
    "        self.position = (self.position + 1) % self.capacity  # loop around memory\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Q-network\n",
    "\n",
    "A Q-network is a neural network that maps states to Q-values for each actions.\n",
    "\n",
    "Implement a first version of Q-networks.\n",
    "Keep it simple (e.g. 3 hidden layers, with Relu activations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, size_hidden, output_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, size_hidden)\n",
    "        #self.bn1 = nn.BatchNorm1d(size_hidden)\n",
    "        \n",
    "        self.fc2 = nn.Linear(size_hidden, size_hidden)   \n",
    "        #self.bn2 = nn.BatchNorm1d(size_hidden)\n",
    "\n",
    "        self.fc3 = nn.Linear(size_hidden, size_hidden)  \n",
    "        #self.bn3 = nn.BatchNorm1d(size_hidden)\n",
    "\n",
    "        self.fc4 = nn.Linear(size_hidden, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x.float()))  # self.bn1()\n",
    "        h2 = F.relu(self.fc2(h1))  # self.bn2()\n",
    "        h3 = F.relu(self.fc3(h2))  # self.bn3()\n",
    "        output = self.fc4(h3) # .view(h3.size(0), -1)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Set up the Q-networks\n",
    "\n",
    "In DQN, the weights of the target network are copied from the weights of policy network every few iterations.\n",
    "\n",
    "We set the frequency of update using TARGET_UPDATE = 10.\n",
    "\n",
    "Instead of RMSprop we will use SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_SIZE = 8\n",
    "HIDDEN_SIZE = 64\n",
    "ACTION_SIZE = 4\n",
    "\n",
    "Q_network = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
    "Q_target = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
    "Q_target.load_state_dict(Q_network.state_dict())\n",
    "Q_target.eval()\n",
    "\n",
    "TARGET_UPDATE = 20\n",
    "\n",
    "optimizer = optim.Adam(Q_network.parameters(), lr=0.001)\n",
    "memory = ReplayMemory(100000)\n",
    "\n",
    "#for p in Q_network.parameters():\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Epsilon-greedy policy\n",
    "\n",
    "You can take inspiration from pytorch tutorial and implement the select_action function.\n",
    "Or, alternatively, you can implement a E-greedy policy class that will select epsilon greedy actions..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E_Greedy_Policy():\n",
    "    \n",
    "    def __init__(self, epsilon, decay, min_epsilon):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay = decay\n",
    "        self.epsilon_min = min_epsilon\n",
    "                \n",
    "    def __call__(self, state):\n",
    "                \n",
    "        is_greedy = random.random() > self.epsilon\n",
    "        if is_greedy :\n",
    "            # we select greedy action\n",
    "            with torch.no_grad():\n",
    "                Q_network.eval()\n",
    "                index_action = Q_network(state).argmax().detach().cpu().numpy().item()\n",
    "                Q_network.train()\n",
    "\n",
    "        else:\n",
    "            # we sample a random action\n",
    "            index_action = env.action_space.sample() # select random action (4 possible values)\n",
    "        \n",
    "        return index_action\n",
    "                \n",
    "    def update_epsilon(self):\n",
    "        \n",
    "        self.epsilon = self.epsilon*self.decay\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        \n",
    "    def reset(self):\n",
    "        self.epsilon = self.epsilon_start\n",
    "        \n",
    "        \n",
    "policy = E_Greedy_Policy(epsilon=0.5, decay=0.997, min_epsilon=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    non_final_next_states = torch.reshape(non_final_next_states, (non_final_mask.sum(), -1))  # Reshape to (nr. non final, 8)\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    state_batch = torch.reshape(state_batch, (BATCH_SIZE, -1))  # Reshape to (batch_size, 8)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q values using policy net\n",
    "    Q_values = Q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute next Q values using Q_targets\n",
    "    next_Q_values = torch.zeros( BATCH_SIZE, device=device)\n",
    "    next_Q_values[non_final_mask] = Q_target(non_final_next_states).max(1)[0].detach()\n",
    "    next_Q_values = next_Q_values.unsqueeze(1)\n",
    "    \n",
    "    # Compute targets\n",
    "    target_Q_values = (next_Q_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Compute MSE Loss\n",
    "    loss = F.mse_loss(Q_values, target_Q_values)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Trick: gradient clipping\n",
    "    for param in Q_network.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x000001C98E4031F0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sdien\\Anaconda3\\envs\\DRL\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 185, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\sdien\\Anaconda3\\envs\\DRL\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 101, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\sdien\\Anaconda3\\envs\\DRL\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 319, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\sdien\\Anaconda3\\envs\\DRL\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 838, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\sdien\\Anaconda3\\envs\\DRL\\lib\\_weakrefset.py\", line 114, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x000001C99D62B590; to 'Win32Window' at 0x000001C98E3F4E80>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the warmup\n",
      "Episode  0 :  reward : -88.23492870604538 eps:  0.4985  loss: tensor(0.1188)\n",
      "Average loss: -8.823492870604538\n",
      "Episode  10 :  reward : -152.88732679026947 eps:  0.48374528580903503  loss: tensor(0.7325)\n",
      "Average loss: -267.51907919134624\n",
      "Episode  20 :  reward : -317.9083484315502 eps:  0.4694272849397492  loss: tensor(0.2673)\n",
      "Average loss: -122.26933310707413\n",
      "Episode  30 :  reward : -349.48619935351746 eps:  0.4555330714538382  loss: tensor(0.1563)\n",
      "Average loss: -222.47882365476607\n",
      "Episode  40 :  reward : -52.870052899134436 eps:  0.4420501019978025  loss: tensor(0.0122)\n",
      "Average loss: -132.0407861167999\n",
      "Episode  50 :  reward : -43.07078411541557 eps:  0.4289662044791175  loss: tensor(0.0767)\n",
      "Average loss: -95.11433643952732\n",
      "Episode  60 :  reward : -105.01173481728462 eps:  0.41626956707756796  loss: tensor(0.1205)\n",
      "Average loss: -95.33160295180252\n",
      "Episode  70 :  reward : -6.636696995642566 eps:  0.40394872758182826  loss: tensor(0.1084)\n",
      "Average loss: -147.17139667549117\n",
      "Episode  80 :  reward : -156.5447378873249 eps:  0.39199256304166  loss: tensor(0.1434)\n",
      "Average loss: -83.28363261891751\n",
      "Episode  90 :  reward : -57.92627821204076 eps:  0.38039027972638717  loss: tensor(0.0235)\n",
      "Average loss: -82.68404897458905\n",
      "Episode  100 :  reward : -251.09717055381873 eps:  0.3691314033805816  loss: tensor(0.2677)\n",
      "Average loss: -112.12217108329519\n",
      "Episode  110 :  reward : 19.990515824098047 eps:  0.35820576976816376  loss: tensor(0.9859)\n",
      "Average loss: -77.65974244552625\n",
      "Episode  120 :  reward : -185.70066879796673 eps:  0.34760351549638074  loss: tensor(0.0707)\n",
      "Average loss: -67.22358676981987\n",
      "Episode  130 :  reward : -162.32086932398923 eps:  0.33731506911137826  loss: tensor(0.1799)\n",
      "Average loss: -95.82221843341776\n",
      "Episode  140 :  reward : -117.79142532949297 eps:  0.3273311424573282  loss: tensor(0.0966)\n",
      "Average loss: -92.85067190465053\n",
      "Episode  150 :  reward : -98.27983328056 eps:  0.3176427222913103  loss: tensor(0.0699)\n",
      "Average loss: -71.73858937957196\n",
      "Episode  160 :  reward : 11.59092944147706 eps:  0.30824106214637886  loss: tensor(0.0868)\n",
      "Average loss: -28.28617140316961\n",
      "Episode  170 :  reward : -62.95490370816577 eps:  0.2991176744354675  loss: tensor(0.0407)\n",
      "Average loss: -92.41666438452019\n",
      "Episode  180 :  reward : -297.4906393862069 eps:  0.2902643227890052  loss: tensor(0.2409)\n",
      "Average loss: -74.377862024509\n",
      "Episode  190 :  reward : 16.43452193484493 eps:  0.28167301461932465  loss: tensor(0.0972)\n",
      "Average loss: -52.5481213234997\n",
      "Episode  200 :  reward : -197.68608162183344 eps:  0.2733359939051508  loss: tensor(0.1592)\n",
      "Average loss: -67.29327198366047\n",
      "Episode  210 :  reward : -72.1667578309395 eps:  0.26524573418965663  loss: tensor(0.1580)\n",
      "Average loss: -72.83617664471743\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_15496/707395513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# Perform one step of the optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m#started_training = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_15496/1013105936.py\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Compute Q values using policy net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mQ_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Compute next Q values using Q_targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_15496/734080318.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# self.bn1()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# self.bn2()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mh3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# self.bn3()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# .view(h3.size(0), -1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")  # create environment\n",
    "\n",
    "num_episodes = 500\n",
    "policy.reset()\n",
    "rewards_history = []\n",
    "\n",
    "# Warmup phase!\n",
    "memory_filled = False\n",
    "\n",
    "while not memory_filled:\n",
    "    \n",
    "    state = env.reset()  # 8 states: coordinates of the lander (x,y), linear velocities (x,y), angle, angular velocity, 2 bools if each leg is touches ground.\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:  # for each episode\n",
    "        # Get action and act in the world\n",
    "        state_tensor = T.from_numpy(state)\n",
    "        action = policy(state_tensor)  # <<--- choose greedy (choose index of highest q-value predicted by network) or exploration\n",
    "        next_state, reward, done, __ = env.step(action)\n",
    "        total_reward += float(reward)\n",
    "        \n",
    "        # Observe new state\n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, float(reward))\n",
    "        state = next_state\n",
    "\n",
    "    memory_filled = memory.capacity == len(memory)\n",
    "\n",
    "print('Done with the warmup')\n",
    "    \n",
    "    \n",
    "for i_episode in range(num_episodes):\n",
    "    # New dungeon at every run\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    \n",
    "    while not done:  # iterate through states\n",
    "        \n",
    "        # Get action and act in the world\n",
    "        state_tensor = T.from_numpy(state)\n",
    "        \n",
    "        action = policy(state_tensor)   # choose greedy (index of q-value predictions) or exploration\n",
    "        next_state, reward, done, __ = env.step(action)\n",
    "        total_reward += float(reward)\n",
    "        \n",
    "        # Observe new state\n",
    "        if done:\n",
    "            next_state = None\n",
    "        memory.push(state, action, next_state, float(reward))  # Store the transition in memory\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "        # Perform one step of the optimization\n",
    "        #started_training = True\n",
    "        loss = optimize_model()\n",
    "\n",
    "    policy.update_epsilon()\n",
    "    rewards_history.append( float(total_reward) )\n",
    "\n",
    "    \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "\n",
    "        Q_target.load_state_dict(Q_network.state_dict())\n",
    "    \n",
    "    # if i_episode % 10 == 0: \n",
    "    #     env.render()\n",
    "    if i_episode % 10 == 0:\n",
    "        print('Episode {} - reward: {:.3f}, eps: {:.3f} loss: {:.3f} avg. loss: {:.3f}'.format(\n",
    "            i_episode, total_reward, policy.epsilon, loss.detach().cpu(), sum(rewards_history[-10:])/10))   \n",
    "\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c9b08a7340>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards_history, 'b.', alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ef810184dd1ed1e7e90889c2154f0f529619798a6f354d5d90c8edd2323b55a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('NC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
